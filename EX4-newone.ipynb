{
 "cells": [
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Setups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To begin, start iPython interface in Inline Pylab mode by typing following on your terminal / windows command prompt:\n",
    "In [1]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After importing the library, you read the dataset using function read_csv(). The file is assumed to be downloaded from the moodle to the data folder in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(arange(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following are the libraries we will use during this task:\n",
    "numpy\n",
    "matplotlib\n",
    "pandas\n",
    "Please note that you do not need to import matplotlib and numpy because of Pylab environment. I have still kept them in the code, in case you use the code in a different environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import matplotlib as plt\n",
    "from collections import Counter\n",
    "from numpy.random import randn\n",
    "#Import models from scikit learn module:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold   #For K-fold cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import roc_auc_score,accuracy_score\n",
    "\n",
    "rcParams['figure.figsize'] = 16, 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "After importing the library, you read the dataset using function read_csv(). The file is assumed to be downloaded from the moodle to the data folder in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/User/Desktop/ML/EX4-data/train.csv\") #Reading the dataset in a dataframe using Pandas\n",
    "test = pd.read_csv(\"C:/Users/User/Desktop/ML/EX4-data/test.csv\") #Reading the dataset in a datafra"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Distribution analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we are familiar with basic data characteristics, let us study distribution of various variables. Let us start with numeric variables – namely ApplicantIncome and LoanAmount\n",
    "\n",
    "Lets start by plotting the histogram of ApplicantIncome using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['ApplicantIncome'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe that there are few extreme values. This is also the reason why 50 bins are required to depict the distribution clearly.\n",
    "Next, we look at box plots to understand the distributions. Box plot can be plotted by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.boxplot(column='ApplicantIncome')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This confirms the presence of a lot of outliers/extreme values. This can be attributed to the income disparity in the society. Part of this can be driven by the fact that we are looking at people with different education levels. Let us segregate them by Education:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.boxplot(column='ApplicantIncome', by = 'Education')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there is no substantial different between the mean income of graduate and non-graduates. But there are a higher number of graduates with very high incomes, which are appearing to be the outliers."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Task 2: Distribution Analysis¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the histogram and boxplot of LoanAmount\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Check yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LoanAmount'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.boxplot(column='LoanAmount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, there are some extreme values. Clearly, both ApplicantIncome and LoanAmount require some amount of data munging. LoanAmount has missing and well as extreme values values, while ApplicantIncome has a few extreme values, which demand deeper understanding. We will take this up in coming sections.\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Categorical variable analysis¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Frequency Table for Credit History:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp1 = train['Credit_History'].value_counts(ascending=True)\n",
    "temp1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of getting loan for each Credit History class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp2 = train.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())\n",
    "temp2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be plotted as a bar chart using the “matplotlib” library with following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.set_xlabel('Credit_History')\n",
    "ax1.set_ylabel('Count of Applicants')\n",
    "ax1.set_title(\"Applicants by Credit_History\")\n",
    "temp1.plot(kind='bar')\n",
    "\n",
    "ax2 = fig.add_subplot(122)\n",
    "temp2.plot(kind = 'bar')\n",
    "ax2.set_xlabel('Credit_History')\n",
    "ax2.set_ylabel('Probability of getting loan')\n",
    "ax2.set_title(\"Probability of getting loan by credit h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "This shows that the chances of getting a loan are eight-fold if the applicant has a valid credit history. You can plot similar graphs by Married, Self-Employed, Property_Area, etc.\n",
    "Alternately, these two plots can also be visualized by combining them in a stacked chart::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp3 = pd.crosstab(train['Credit_History'], train['Loan_Status'])\n",
    "temp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We just saw how we can do exploratory analysis in Python using Pandas. I hope your love for pandas (the animal) would have increased by now – given the amount of help, the library can provide you in analyzing datasets.\n",
    "Next let’s explore ApplicantIncome and LoanStatus variables further, perform data munging and create a dataset for applying various modeling techniques. I would strongly urge that you take another dataset and problem and go through an independent example before reading further."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Data Munging in Python : Using Pandas\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Data munging – recap of the need\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While our exploration of the data, we found a few problems in the data set, which needs to be solved before the data is ready for a good model. This exercise is typically referred as “Data Munging”. Here are the problems, we are already aware of:\n",
    "1. There are missing values in some variables. We should estimate those values wisely depending on the amount of missing values and the expected importance of variables.\n",
    "2. While looking at the distributions, we saw that ApplicantIncome and LoanAmount seemed to contain extreme values at either end. Though they might make intuitive sense, but should be treated appropriately.\n",
    "\n",
    "In addition to these problems with numerical fields, we should also look at the non-numerical fields i.e. Gender, Property_Area, Married, Education and Dependents to see, if they contain any useful information."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Check missing values in the dataset¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at missing values in all the variables because most of the models don’t work with missing data and even if they do, imputing them helps more often than not. So, let us check the number of nulls / NaNs in the dataset.\n",
    "This command should tell us the number of missing values in each column as isnull() returns 1, if the value is null."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.apply(lambda x: sum(x.isnull()),axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Though the missing values are not very high in number, but many variables have them and each one of these should be estimated and added in the data.\n",
    "\n",
    "Note: Remember that missing values may not always be NaNs. For instance, if the Loan_Amount_Term is 0, does it makes sense or would you consider that missing? I suppose your answer is missing and you’re right. So we should check for values which are unpractical."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "How to fill missing values in LoanAmount?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to fill missing values in LoanAmount?\n",
    "There are numerous ways to fill the missing values of loan amount – the simplest being replacement by mean, which can be done by following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Self_Employed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The other extreme could be to build a supervised learning model to predict loan amount on the basis of other variables and then use age along with other variables to predict survival.\n",
    "\n",
    "Since, the purpose now is to bring out the steps in data munging, I’ll rather take an approach, which lies some where in between these 2 extremes. A key hypothesis is that whether a person is educated or self-employed can combine to give a good estimate of loan amount.\n",
    "\n",
    "But first, we have to ensure that each of Self_Employed and Education variables should not have a missing values.\n",
    "\n",
    "As we say earlier, Self_Employed has some missing values. Let’s look at the frequency table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Self_Employed'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since ~86% values are “No”, it is safe to impute the missing values as “No” as there is a high probability of success. This can be done using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Self_Employed'].fillna('No',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will create a Pivot table, which provides us median values for all the groups of unique values of Self_Employed and Education features. Next, we define a function, which returns the values of these cells and apply it to fill the missing values of loan amount:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = train.pivot_table(values='LoanAmount', index='Self_Employed' ,columns='Education', aggfunc=np.median)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function to return value of this pivot_table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fage(x):\n",
    " return table.loc[x['Self_Employed'],x['Education']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing values:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LoanAmount'].fillna(train[train['LoanAmount'].isnull()].apply(fage, axis=1), inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This should provide you a good way to impute missing values of loan amount.\n"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "How to treat for extreme values in distribution of LoanAmount and ApplicantIncome?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s analyze LoanAmount first. Since the extreme values are practically possible, i.e. some people might apply for high value loans due to specific needs. So instead of treating them as outliers, let’s try a log transformation to nullify their effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['LoanAmount_log'] = np.log(train['LoanAmount'])\n",
    "train['LoanAmount_log'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the distribution looks much closer to normal and effect of extreme values has been significantly subsided.\n",
    "Coming to ApplicantIncome. One intuition can be that some applicants have lower income but strong support Co-applicants. So it might be a good idea to combine both incomes as total income and take a log transformation of the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['TotalIncome'] = train['ApplicantIncome'] + train['CoapplicantIncome']\n",
    "train['TotalIncome_log'] = np.log(train['TotalIncome'])\n",
    "train['LoanAmount_log'].hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we see that the distribution is much better than before."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Task 3: Remove nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Impute the missing values for Gender, Married, Dependents, Loan_Amount_Term, Credit_History\n",
    "2. Also, I encourage you to think about possible additional information which can be derived from the data. For example, creating a column for LoanAmount/TotalIncome might make sense as it gives an idea of how well the applicant is suited to pay back his loan."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "check Yourself¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Loan_Amount_Term'].fillna(360, inplace=True)\n",
    "train['Credit_History'].fillna(1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Dependents'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Dependents'].fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Married'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Married'].fillna('Yes', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Gender'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train['Married'].fillna('Male', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will look at making predictive models."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Building a Predictive Model in Python¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After, we have made the data useful for modeling, let’s now look at the python code to create a predictive model on our data set. Skicit-Learn (sklearn) is the most commonly used library in Python for this purpose and we will follow the trail.\n",
    "\n",
    "Since, sklearn requires all inputs to be numeric, we should convert all our categorical variables into numeric by encoding the categories. This can be done using the following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "var_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    train[i] = le.fit_transform(train[i].astype(str))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will import the required modules. Then we will define a generic classification function, which takes a model as input and determines the Accuracy and Cross-Validation scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import models from scikit learn module:\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import KFold   #For K-fold cross validation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "from sklearn import metrics\n",
    "\n",
    "#Generic function for making a classification model and accessing performance:\n",
    "def classification_model(model, data, predictors, outcome):\n",
    "  #Fit the model:\n",
    "  model.fit(data[predictors],data[outcome])\n",
    "  \n",
    "  #Make predictions on training set:\n",
    "  predictions = model.predict(data[predictors])\n",
    "  \n",
    "  #Print accuracy\n",
    "  accuracy = metrics.accuracy_score(predictions,data[outcome])\n",
    "  print(\"Accuracy : %s\" % \"{0:.3%}\".format(accuracy))\n",
    "\n",
    "  #Perform k-fold cross-validation with 5 folds\n",
    "  kf = KFold(data.shape[0], n_folds=5)\n",
    "  error = []\n",
    "  for train, test in kf:\n",
    "    # Filter training data\n",
    "    train_predictors = (data[predictors].iloc[train,:])\n",
    "    \n",
    "    # The target we're using to train the algorithm.\n",
    "    train_target = data[outcome].iloc[train]\n",
    "    \n",
    "    # Training the algorithm using the predictors and target.\n",
    "    model.fit(train_predictors, train_target)\n",
    "    \n",
    "    #Record error from each cross-validation run\n",
    "    error.append(model.score(data[predictors].iloc[test,:], data[outcome].iloc[test]))\n",
    " \n",
    "  print(\"Cross-Validation Score : %s\" % \"{0:.3%}\".format(np.mean(error)))\n",
    "\n",
    "  #Fit the model again so that it can be refered outside the function:\n",
    "  model.fit(data[predictors],data[outcome])"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Logistic Regression¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s make our first Logistic Regression model. One way would be to take all the variables into the model but this might result in overfitting. In simple words, taking all variables might result in the model understanding complex relations specific to the data and will not generalize well.\n",
    "We can easily make some intuitive hypothesis to set the ball rolling. The chances of getting a loan will be higher for:\n",
    "1. Applicants having a credit history (remember we observed this in exploration?)\n",
    "2. Applicants with higher applicant and co-applicant incomes\n",
    "3. Applicants with higher education level\n",
    "4. Properties in urban areas with high growth perspectives\n",
    "So let’s make our first model with ‘Credit_History’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outcome_var = 'Loan_Status'\n",
    "model = LogisticRegression()\n",
    "predictor_var = ['Credit_History']\n",
    "classification_model(model, train,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try different combination of variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_var = ['Credit_History','Education','Married','Self_Employed','Property_Area']\n",
    "classification_model(model, train,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally we expect the accuracy to increase on adding variables. But this is a more challengin case. The accuracy and cross-validation score are not getting impacted by less important variables. Credit_History is dominating the mode. We have two options now:\n",
    "1. Feature Engineering: dereive new information and try to predict those. I will leave this to your creativity.\n",
    "2. Better modeling techniques. Let’s explore this next."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    "Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Decision tree is another method for making a predictive model. It is known to provide higher accuracy than logistic regression model.\n",
    "In [35]:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DecisionTreeClassifier()\n",
    "predictor_var = ['Credit_History','Gender','Married','Education']\n",
    "classification_model(model, train,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the model based on categorical variables is unable to have an impact because Credit History is dominating over them. Let’s try a few numerical variables:"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Task 4: Modeling the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Train a decision tree classifier that accepts the following attributes as input: Credit_History, Loan_Amount_Term, LoanAmount_log\n",
    "2. Is it better that the previous decision tree?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = DecisionTreeClassifier()\n",
    "predictor_var = ['Credit_History','Loan_Amount_Term', 'LoanAmount_log']\n",
    "classification_model(model, train, predictor_var, outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy is significantly better, but cross-validation is worse. In overall performance it might be worst than the prevoius one, as it seems like an overfitting issue."
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 2,
   "source": [
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We can try different combination of variables:\n",
    "predictor_var = ['Credit_History','Loan_Amount_Term','LoanAmount_log']\n",
    "classification_model(model, train,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Not much better according to the test score\n",
    "Here we observed that although the accuracy went up on adding variables, the cross-validation error went down. This is the result of model over-fitting the data. Let’s try an even more sophisticated algorithm and see if it helps:"
   ]
  },
  {
   "cell_type": "heading",
   "metadata": {},
   "level": 1,
   "source": [
    "Random Forest¶\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Random forest is another algorithm for solving the classification problem.\n",
    "An advantage with Random Forest is that we can make it work with all the features and it returns a feature importance matrix which can be used to select features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=100)\n",
    "predictor_var = ['Gender', 'Married', 'Dependents', 'Education',\n",
    "       'Self_Employed', 'Loan_Amount_Term', 'Credit_History', 'Property_Area',\n",
    "        'LoanAmount_log','TotalIncome_log']\n",
    "classification_model(model, train,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy : 100.000%\n",
    "Cross-Validation Score : 77.856%\n",
    "Here we see that the accuracy is 100% for the training set. This is the ultimate case of overfitting and can be resolved in two ways:\n",
    "Reducing the number of predictors\n",
    "Tuning the model parameters\n",
    "Let’s try both of these. First we see the feature importance matrix from which we’ll take the most important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create a series with feature importances:\n",
    "featimp = pd.Series(model.feature_importances_, index=predictor_var).sort_values(ascending=False)\n",
    "print(featimp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use the top 5 variables for creating a model. Also, we will modify the parameters of random forest model a little bit:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s use the top 5 variables for creating a model. Also, we will modify the parameters of random forest model a little bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=25, min_samples_split=25, max_depth=7, max_features=1)\n",
    "predictor_var = ['TotalIncome_log','LoanAmount_log','Credit_History','Dependents','Property_Area']\n",
    "classification_model(model, train,predictor_var,outcome_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we start.\n",
    "We saw erlier the columns with missing values:\n",
    "Gender 13\n",
    "Married 3\n",
    "Depentents 15\n",
    "Self_Employed 32\n",
    "LoanAmount 22\n",
    "Loan_Amount_Term 14\n",
    "Credit_History 50\n",
    "\n",
    "lets check what is going with the train set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will divide the train set into two,one with the wanted column and one without.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"C:/Users/moran beladev/Desktop/Studies/IDF/data/train.csv\")\n",
    "test = pd.read_csv(\"C:/Users/moran beladev/Desktop/Studies/IDF/data/test.csv\")\n",
    "\n",
    "trainX = train.drop('Loan_Status', axis=1, inplace=False)\n",
    "trainX = train.drop('Loan_Status', axis=1, inplace=False)\n",
    "\n",
    "testX = test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the values in these ocolumns: Married, depentdents and Gender since it is more easy to fill those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX['Married'].value_counts(),'\\n')\n",
    "print(trainX['Dependents'].value_counts(),'\\n')\n",
    "print(trainX['Gender'].value_counts(),'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Now, we will do it on the test too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(testX['Married'].value_counts(),'\\n')\n",
    "print(testX['Dependents'].value_counts(),'\\n')\n",
    "print(testX['Gender'].value_counts(),'\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We fill in the missing value with the most accured one "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX['Dependents'].fillna(0, inplace=True)\n",
    "trainX['Married'].fillna('Yes', inplace=True)\n",
    "trainX['Gender'].fillna('Male', inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now lets take care of the rest missing values of 'Loan_amount_term', 'loanAmount'and\n",
    "'Credit_History' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX['LoanAmount'].fillna(trainX.LoanAmount.dropna().mode().values[0], inplace=True)\n",
    "trainX['Loan_Amount_Term'].fillna(trainX.Loan_Amount_Term.dropna().mode().values[0], inplace=True)\n",
    "trainX['Self_Employed'].fillna(trainX.Self_Employed.dropna().mode().values[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the same for testX set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX['Dependents'].fillna(0, inplace=True)\n",
    "testX['Married'].fillna('Yes', inplace=True)\n",
    "testX['Gender'].fillna('Male', inplace=True)\n",
    "#please note we fill in the missing values based on the training set\n",
    "testX['LoanAmount'].fillna(trainX.LoanAmount.dropna().mode().values[0], inplace=True)\n",
    "testX['Loan_Amount_Term'].fillna(trainX.Loan_Amount_Term.dropna().mode().values[0], inplace=True)\n",
    "testX['Self_Employed'].fillna(trainX.Self_Employed.dropna().mode().values[0], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now there is only the 'Credit_history' feature we still need to handle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX['Credit_History'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for this feature we will train the set, let's combine the two sets into one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "var_mod = {'Gender', 'Married', 'Dependents', 'Education', 'Self_Employed', 'Property_Area'}\n",
    "le = LabelEncoder()\n",
    "for i in var_mod:\n",
    "    trainX[i] = le.fit_transform(trainX[i].astype(str))\n",
    "    testX[i] = le.fit_transform(testX[i].astype(str))\n",
    "trainY=le.fit_transform(trainY.astype(str))\n",
    "tmp_trainX = trainX[trainX.Credit_History.notnull()]\n",
    "randomForest = RandomForestClassifier(random_state=np.random)\n",
    "randomForest.fit(tmp_trainX.drop(['Credit_History', 'Loan_ID'], axis=1), tmp_trainX['Credit_History'])\n",
    "\n",
    "testX_i = testX[testX.Credit_History.isnull()]\n",
    "testY_Predict = randomForest.predict(testX_i.drop(['Credit_History', 'Loan_ID','Loan_Status'], axis=1))\n",
    "testX.loc[testX.Credit_History.isnull(),'Credit_History'] = testY_Predict\n",
    "\n",
    "trainX_i = trainX[trainX.Credit_History.isnull()]\n",
    "trainY_Predict = randomForest.predict(trainX_i.drop(['Credit_History', 'Loan_ID'], axis=1))\n",
    "trainX.loc[trainX.Credit_History.isnull(),'Credit_History'] = trainY_Predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use KNN Classifier - K nearest neighbors algorithm\n",
    "K Nearest Neighbors  Classification is a simple algorithm that stores all available cases and classifies new cases\n",
    "based on a similarity measure (e.g., distance functions). KNN has been used in statistical estimation and pattern\n",
    "recognition already in the beginning of 1970’s as a non-parametric technique\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNNClassifier = KNeighborsClassifier()\n",
    "KNN = KNeighborsClassifier(n_neighbors=5, weights='uniform', algorithm='auto', p=2)\n",
    "KNN.fit(trainX.drop(['Loan_ID'],axis=1), trainY)\n",
    "\n",
    "loan_ids=testX['Loan_ID']\n",
    "result = pd.DataFrame({'Loan_ID': loan_ids.values, 'Loan_Status': KNN.predict(testX.drop(['Loan_ID','Loan_Status'],axis=1))})\n",
    "result.to_csv('submission1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we will use Random Forest Classifier -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaboostClassifier (n_estimators=50, learning_rate=0.5, random_state = 11 )\n",
    "abc.fit()trainX.drop(['Loan_ID'],axis=1), trainY)\n",
    "\n",
    "loan_ids=testX['Loan_ID']\n",
    "result = pd.DataFrame({'Loan_ID': loan_ids.values, 'Loan_Status': abc.predict(testX.drop(['Loan_ID','Loan_Status'],axis=1))})\n",
    "result.to_csv('submission2.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2.0
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}